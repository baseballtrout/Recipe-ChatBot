{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6041428,"sourceType":"datasetVersion","datasetId":3454349}],"dockerImageVersionId":30587,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center>\n\n<h1>Building Chatbot for Interactive and Interactive Recipe recommendation</h1>\n<h3>MSDS 560 - Natural Language PRocessing Project</h3>\n<h3>Group A - Bradford Patton and Christin Oguno</h3>\n\n</center>\n\n<h2>Project Description</h2>\n\nSpecific Aims: \n    1. Matching the recipes based on user preferences. \n\nA large dataset of recipes is provided to build your NLP pipeline. \n\nTasks:\n1. Build a Chatbot that accepts user input in multiple iterations. \n2. Retrieve recipes based on user input and \n3. filter in the sub-sequent iteration.\nOutput.\n\nIn a iterative process, this chatbot will help a user gather recipes of their preferences. Final output is a list of recipes that satisfies the user criteria. \n\n\nYourRecipeBot.com is new start-up that looking to improve the user experience around recipe search. They have identified that typically users having difficult time find recipes right for their diet. \n\nThis new company is building a new chatBot for interactive recipe search. The chatBot will interact with the user. It will take input about recipe preference and provide recommendation of recipes. User get the chance refine the search in subsequence interaction by putting new search criteria. And the chatBot again filter the recommendation. Over a brief interaction, the company hope that user will get what she is looking for. \n\nThey need you help. \n\nThey are using publicly available recipe databases for training and testing the model before moving to production level implementation. \n\nRelevant Article:\n\nhttps://aclanthology.org/D19-1613/ ","metadata":{}},{"cell_type":"markdown","source":"## Load Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nrecipe=pd.read_csv('/kaggle/input/recipe-dataset-over-2m/recipes_data.csv')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-13T00:15:18.698579Z","iopub.execute_input":"2023-12-13T00:15:18.699060Z","iopub.status.idle":"2023-12-13T00:16:20.583428Z","shell.execute_reply.started":"2023-12-13T00:15:18.699010Z","shell.execute_reply":"2023-12-13T00:16:20.582218Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recipe.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:16:23.821032Z","iopub.execute_input":"2023-12-13T00:16:23.821452Z","iopub.status.idle":"2023-12-13T00:16:23.851747Z","shell.execute_reply.started":"2023-12-13T00:16:23.821421Z","shell.execute_reply":"2023-12-13T00:16:23.850736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recipe.ingredients[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:16:27.353025Z","iopub.execute_input":"2023-12-13T00:16:27.353403Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Working with only a small version of the dataset","metadata":{}},{"cell_type":"code","source":"recipe_small=recipe.head(500) ","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:16:30.909902Z","iopub.execute_input":"2023-12-13T00:16:30.910992Z","iopub.status.idle":"2023-12-13T00:16:30.915726Z","shell.execute_reply.started":"2023-12-13T00:16:30.910943Z","shell.execute_reply":"2023-12-13T00:16:30.914579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recipe_small.count()","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:16:33.557077Z","iopub.execute_input":"2023-12-13T00:16:33.557542Z","iopub.status.idle":"2023-12-13T00:16:33.573332Z","shell.execute_reply.started":"2023-12-13T00:16:33.557505Z","shell.execute_reply":"2023-12-13T00:16:33.572022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recipe_small.title.duplicated().tail(90)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:05:00.157005Z","iopub.execute_input":"2023-12-11T16:05:00.157686Z","iopub.status.idle":"2023-12-11T16:05:00.170563Z","shell.execute_reply.started":"2023-12-11T16:05:00.157641Z","shell.execute_reply":"2023-12-11T16:05:00.168876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"duplicated_titles = recipe_small[recipe_small.duplicated(subset='title', keep=False)]\nprint(\"Rows with Duplicated Titles:\")\nprint(duplicated_titles)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:06:27.914957Z","iopub.execute_input":"2023-12-11T16:06:27.915444Z","iopub.status.idle":"2023-12-11T16:06:27.938165Z","shell.execute_reply.started":"2023-12-11T16:06:27.915411Z","shell.execute_reply":"2023-12-11T16:06:27.936923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"title_counts = recipe_small['title'].value_counts()\nduplicated_title_counts = title_counts[title_counts > 1]\nprint(\"\\nDuplicated Titles and Their Counts:\")\nprint(duplicated_title_counts)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T16:07:36.194770Z","iopub.execute_input":"2023-12-11T16:07:36.195239Z","iopub.status.idle":"2023-12-11T16:07:36.211333Z","shell.execute_reply.started":"2023-12-11T16:07:36.195204Z","shell.execute_reply":"2023-12-11T16:07:36.209952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recipe_small.NER[0]","metadata":{"execution":{"iopub.status.busy":"2023-12-10T22:04:53.825690Z","iopub.execute_input":"2023-12-10T22:04:53.827120Z","iopub.status.idle":"2023-12-10T22:04:53.835030Z","shell.execute_reply.started":"2023-12-10T22:04:53.827004Z","shell.execute_reply":"2023-12-10T22:04:53.833546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recipe_small.index","metadata":{"execution":{"iopub.status.busy":"2023-12-10T21:30:31.029401Z","iopub.execute_input":"2023-12-10T21:30:31.029791Z","iopub.status.idle":"2023-12-10T21:30:31.037804Z","shell.execute_reply.started":"2023-12-10T21:30:31.029763Z","shell.execute_reply":"2023-12-10T21:30:31.036300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## *Using NER column because I want to match recipes based on key words which is that column","metadata":{}},{"cell_type":"code","source":"indices = pd.Series(recipe_small.index, index = recipe_small['NER'])\nindices","metadata":{"execution":{"iopub.status.busy":"2023-12-10T22:05:03.962122Z","iopub.execute_input":"2023-12-10T22:05:03.962524Z","iopub.status.idle":"2023-12-10T22:05:03.972313Z","shell.execute_reply.started":"2023-12-10T22:05:03.962495Z","shell.execute_reply":"2023-12-10T22:05:03.970457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recipe_small.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-10T22:05:11.450019Z","iopub.execute_input":"2023-12-10T22:05:11.451085Z","iopub.status.idle":"2023-12-10T22:05:11.456326Z","shell.execute_reply.started":"2023-12-10T22:05:11.450955Z","shell.execute_reply":"2023-12-10T22:05:11.455454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"A recommendation system takes an input (words) or activities (online shopping searches) from you \n\nYou want to get the RECIPES that contain these words\n\nBased on similarity metrics- cosign similarity \n","metadata":{}},{"cell_type":"markdown","source":"## Data Cleaning","metadata":{}},{"cell_type":"code","source":"import nltk # natural language toolkit\nnltk.download('all',  halt_on_error=False)\nimport spacy\n\nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport re\nfrom bs4 import BeautifulSoup # for cleaning addidtional text from document \nimport unicodedata","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:16:53.325656Z","iopub.execute_input":"2023-12-13T00:16:53.326108Z","iopub.status.idle":"2023-12-13T00:17:21.609568Z","shell.execute_reply.started":"2023-12-13T00:16:53.326074Z","shell.execute_reply":"2023-12-13T00:17:21.607725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nCONTRACTION_MAP = {\n\"ain't\": \"is not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"don't\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he would\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"I'd\": \"I would\",\n\"I'd've\": \"I would have\",\n\"I'll\": \"I will\",\n\"I'll've\": \"I will have\",\n\"I'm\": \"I am\",\n\"I've\": \"I have\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so as\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\"\n}","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:17:31.534913Z","iopub.execute_input":"2023-12-13T00:17:31.535366Z","iopub.status.idle":"2023-12-13T00:17:31.553457Z","shell.execute_reply.started":"2023-12-13T00:17:31.535335Z","shell.execute_reply":"2023-12-13T00:17:31.551922Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\") # Initialize Spacy for english language.\ntokenizer = ToktokTokenizer() # Initialize the nltk tokenizer\nstopword_list = nltk.corpus.stopwords.words('english') # populate the stop-word list from nltk.\nstopword_list.remove('no')   # remove 'no' from the stop-word. Because no is important\nstopword_list.remove('not')  # Similarly remove 'not' from stop-word. ","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:17:37.189028Z","iopub.execute_input":"2023-12-13T00:17:37.189422Z","iopub.status.idle":"2023-12-13T00:17:38.644793Z","shell.execute_reply.started":"2023-12-13T00:17:37.189394Z","shell.execute_reply":"2023-12-13T00:17:38.643740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")  ## calling BeautifulSoup function to process text\n    stripped_text = soup.get_text()  ## calling get_text to get the cleaned text\n    return stripped_text   ## return the clean text\n\ndef remove_stopwords(text, is_lower_case=False):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    if is_lower_case:\n        filtered_tokens = [token for token in tokens if token not in stopword_list]\n    else:\n        filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text\ndef lemmatize_text(text):\n    text = nlp(text)\n    text = ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n    return text\n\ndef remove_special_characters(text):\n    text = re.sub('[^a-zA-Z0-9\\s]', '', text)   ## Remove the characters other than a-z A-Z 0-9 and space. \n    return text\n\ndef expand_contractions(text, contraction_mapping=CONTRACTION_MAP):\n    \n    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n                                      flags=re.IGNORECASE|re.DOTALL)\n    def expand_match(contraction):\n        match = contraction.group(0)\n        first_char = match[0]\n        expanded_contraction = contraction_mapping.get(match) \\\n                                   if contraction_mapping.get(match) \\\n                                    else contraction_mapping.get(match.lower())                       \n        expanded_contraction = first_char+expanded_contraction[1:]\n        return expanded_contraction\n        \n    expanded_text = contractions_pattern.sub(expand_match, text)\n    expanded_text = re.sub(\"'\", \"\", expanded_text)\n    return expanded_text\n\nimport re\n\ndef convert_func(matchobj):\n    m =  matchobj.group(0)\n    map = {'7': 'seven',\n           '8': 'eight',\n           '9': 'nine'}\n    return map[m]\ndef remove_accented_chars(text):\n    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n    return text\ndef strip_html_tags(text):\n    soup = BeautifulSoup(text, \"html.parser\")  ## calling BeautifulSoup function to process text\n    stripped_text = soup.get_text()  ## calling get_text to get the cleaned text\n    return stripped_text ","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:17:40.393830Z","iopub.execute_input":"2023-12-13T00:17:40.395005Z","iopub.status.idle":"2023-12-13T00:17:40.414254Z","shell.execute_reply.started":"2023-12-13T00:17:40.394963Z","shell.execute_reply":"2023-12-13T00:17:40.412948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def normalize_corpus(corpus, html_stripping= True, contraction_expansion=True,\n                     accented_char_removal=True, text_lower_case=True, \n                     text_lemmatization=True, special_char_removal=True, \n                     stopword_removal=True):\n    \n    normalized_corpus = []\n    \n    for doc in corpus:\n        \n        if html_stripping:\n            doc = strip_html_tags(doc)\n        \n        if accented_char_removal:\n            doc = remove_accented_chars(doc)\n            \n        if contraction_expansion:\n            doc = expand_contractions(doc)\n            \n        if text_lower_case:\n            doc = doc.lower()\n            \n        # remove extra newlines\n        doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n        # insert spaces between special characters to isolate them    \n        special_char_pattern = re.compile(r'([{.(-)!}])')\n        doc = special_char_pattern.sub(\" \\\\1 \", doc)\n        \n        if text_lemmatization:\n            doc = lemmatize_text(doc)\n            \n        if special_char_removal:\n            doc = remove_special_characters(doc)  \n            \n        # remove extra whitespace\n        doc = re.sub(' +', ' ', doc)\n        \n        if stopword_removal:\n            doc = remove_stopwords(doc, is_lower_case=text_lower_case)\n            \n        normalized_corpus.append(doc)\n        \n    return normalized_corpus","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:17:44.661627Z","iopub.execute_input":"2023-12-13T00:17:44.662024Z","iopub.status.idle":"2023-12-13T00:17:44.672742Z","shell.execute_reply.started":"2023-12-13T00:17:44.661995Z","shell.execute_reply":"2023-12-13T00:17:44.671415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#created the clean data // doing the work // but do we need more columns? \nThis only cleaned NER column","metadata":{}},{"cell_type":"code","source":"#type(recipes)","metadata":{"execution":{"iopub.status.busy":"2023-12-06T04:54:51.489407Z","iopub.execute_input":"2023-12-06T04:54:51.489707Z","iopub.status.idle":"2023-12-06T04:54:51.505078Z","shell.execute_reply.started":"2023-12-06T04:54:51.489682Z","shell.execute_reply":"2023-12-06T04:54:51.504031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Clean all necessary columns: NER, ingredients, directions, and title ","metadata":{}},{"cell_type":"markdown","source":"## Add all cleaned to columns to OG dataset","metadata":{}},{"cell_type":"markdown","source":"We need to take this cleaned columns and add them to our dataframe so we can use it for further processing steps","metadata":{}},{"cell_type":"code","source":"recipe_small['cleaned_NER']= normalize_corpus(recipe_small['NER'])\nrecipe_small['cleaned_ingredients']= normalize_corpus(recipe_small['ingredients'])\nrecipe_small['cleaned_directions']= normalize_corpus(recipe_small['directions'])\nrecipe_small['cleaned_title']= normalize_corpus(recipe_small['title'])","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:17:48.681760Z","iopub.execute_input":"2023-12-13T00:17:48.682208Z","iopub.status.idle":"2023-12-13T00:18:19.520219Z","shell.execute_reply.started":"2023-12-13T00:17:48.682171Z","shell.execute_reply":"2023-12-13T00:18:19.518824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recipe_small.tail(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:22:17.608297Z","iopub.execute_input":"2023-12-11T22:22:17.608766Z","iopub.status.idle":"2023-12-11T22:22:17.629192Z","shell.execute_reply.started":"2023-12-11T22:22:17.608726Z","shell.execute_reply":"2023-12-11T22:22:17.627884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recipe_small['new_title'] = (\n    recipe_small['cleaned_title'] + recipe_small.groupby('cleaned_title')\n    .cumcount()\n    .astype(str)\n    .replace('0', '') \n)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:18:41.810434Z","iopub.execute_input":"2023-12-13T00:18:41.810896Z","iopub.status.idle":"2023-12-13T00:18:41.832528Z","shell.execute_reply.started":"2023-12-13T00:18:41.810858Z","shell.execute_reply":"2023-12-13T00:18:41.831705Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Tfidf Vectorizing recommendation","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:18:46.405054Z","iopub.execute_input":"2023-12-13T00:18:46.405812Z","iopub.status.idle":"2023-12-13T00:18:46.411821Z","shell.execute_reply.started":"2023-12-13T00:18:46.405757Z","shell.execute_reply":"2023-12-13T00:18:46.410405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer(analyzer= 'word', ngram_range=(1,5), min_df=0.002, max_df=0.5, max_features=5000)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:18:48.593137Z","iopub.execute_input":"2023-12-13T00:18:48.593897Z","iopub.status.idle":"2023-12-13T00:18:48.599129Z","shell.execute_reply.started":"2023-12-13T00:18:48.593856Z","shell.execute_reply":"2023-12-13T00:18:48.597877Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ngram range defines dictinoary\ndirections to ngram (1,5) and kept the max features the same \nnext we will concatnate all the rows we need in our recommedation system","metadata":{}},{"cell_type":"code","source":"recipe_small2= recipe_small['new_title']+\"\"+recipe_small['cleaned_ingredients']+\"\"+recipe_small['cleaned_directions']+\"\"+recipe_small['cleaned_NER']\n","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:18:51.473452Z","iopub.execute_input":"2023-12-13T00:18:51.473941Z","iopub.status.idle":"2023-12-13T00:18:51.485051Z","shell.execute_reply.started":"2023-12-13T00:18:51.473895Z","shell.execute_reply":"2023-12-13T00:18:51.483312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix = tfidf.fit_transform(recipe_small2)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:19:10.785786Z","iopub.execute_input":"2023-12-13T00:19:10.786203Z","iopub.status.idle":"2023-12-13T00:19:11.336305Z","shell.execute_reply.started":"2023-12-13T00:19:10.786174Z","shell.execute_reply":"2023-12-13T00:19:11.335270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:19:13.501079Z","iopub.execute_input":"2023-12-13T00:19:13.501472Z","iopub.status.idle":"2023-12-13T00:19:13.508967Z","shell.execute_reply.started":"2023-12-13T00:19:13.501442Z","shell.execute_reply":"2023-12-13T00:19:13.507731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(tfidf.vocabulary_)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:18:46.563853Z","iopub.execute_input":"2023-12-11T22:18:46.564290Z","iopub.status.idle":"2023-12-11T22:18:46.569504Z","shell.execute_reply.started":"2023-12-11T22:18:46.564256Z","shell.execute_reply":"2023-12-11T22:18:46.568354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 3: Copmuting Pair-wise similarity of articles: once we get vectors compute the similarity between recipes - for EACH recipe we want to find the similarity score comparing with ALL the other recipes in the dataset = we will have the simlarity values of each recipe to EACH recipe ","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:19:17.184952Z","iopub.execute_input":"2023-12-13T00:19:17.185377Z","iopub.status.idle":"2023-12-13T00:19:17.190886Z","shell.execute_reply.started":"2023-12-13T00:19:17.185346Z","shell.execute_reply":"2023-12-13T00:19:17.189558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_matrix=cosine_similarity(tfidf_matrix, tfidf_matrix)","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:19:19.365920Z","iopub.execute_input":"2023-12-13T00:19:19.366597Z","iopub.status.idle":"2023-12-13T00:19:19.389894Z","shell.execute_reply.started":"2023-12-13T00:19:19.366565Z","shell.execute_reply":"2023-12-13T00:19:19.388744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:19:22.157261Z","iopub.execute_input":"2023-12-13T00:19:22.157657Z","iopub.status.idle":"2023-12-13T00:19:22.165052Z","shell.execute_reply.started":"2023-12-13T00:19:22.157627Z","shell.execute_reply":"2023-12-13T00:19:22.164022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#comparting the first row with the first row\nsimilarity_matrix[0][0]","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:04:31.731518Z","iopub.execute_input":"2023-12-11T22:04:31.731948Z","iopub.status.idle":"2023-12-11T22:04:31.740053Z","shell.execute_reply.started":"2023-12-11T22:04:31.731916Z","shell.execute_reply":"2023-12-11T22:04:31.738648Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(recipe_small['cleaned_title'][0])\nprint(recipe_small['cleaned_NER'][1])\nprint(similarity_matrix[0][1])","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:04:37.782566Z","iopub.execute_input":"2023-12-11T22:04:37.782987Z","iopub.status.idle":"2023-12-11T22:04:37.789506Z","shell.execute_reply.started":"2023-12-11T22:04:37.782952Z","shell.execute_reply":"2023-12-11T22:04:37.788399Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accepting User Input:","metadata":{}},{"cell_type":"markdown","source":"## Recommending based on Title","metadata":{}},{"cell_type":"code","source":"indices1 = pd.Series(recipe_small.index, index = recipe_small['new_title'])\nindices1","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:19:26.564978Z","iopub.execute_input":"2023-12-13T00:19:26.565385Z","iopub.status.idle":"2023-12-13T00:19:26.575580Z","shell.execute_reply.started":"2023-12-13T00:19:26.565357Z","shell.execute_reply":"2023-12-13T00:19:26.574427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The different values represent the different similarity values. We need to get the indexes for each of these so we can link it to the data to get actual recipe titles. ","metadata":{}},{"cell_type":"code","source":"def recommendations(user_input, topk=10):\n    idx=indices1[user_input]\n    sim_values=similarity_matrix[idx]\n    sim_values=list(enumerate(sim_values))\n    sim_values=sorted(sim_values, key=lambda x:x[1], reverse=True)\n    ## getting the titles by indices. \n    content_index=[i[0] for i in sim_values[1:topk+1]]\n    recom = recipe_small[['new_title', 'ingredients', 'directions', 'link']].iloc[content_index, :]\n    return recom","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:19:31.865074Z","iopub.execute_input":"2023-12-13T00:19:31.865459Z","iopub.status.idle":"2023-12-13T00:19:31.873402Z","shell.execute_reply.started":"2023-12-13T00:19:31.865428Z","shell.execute_reply":"2023-12-13T00:19:31.872044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#recommendations('log cabin toast')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:09:45.755488Z","iopub.execute_input":"2023-12-11T23:09:45.756005Z","iopub.status.idle":"2023-12-11T23:09:45.761636Z","shell.execute_reply.started":"2023-12-11T23:09:45.755961Z","shell.execute_reply":"2023-12-11T23:09:45.760509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommendations('bake bean')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:27:29.630810Z","iopub.execute_input":"2023-12-13T00:27:29.631233Z","iopub.status.idle":"2023-12-13T00:27:29.646857Z","shell.execute_reply.started":"2023-12-13T00:27:29.631203Z","shell.execute_reply":"2023-12-13T00:27:29.645898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"recommendations('skillet beef macaroni')","metadata":{"execution":{"iopub.status.busy":"2023-12-13T00:22:18.211747Z","iopub.execute_input":"2023-12-13T00:22:18.212175Z","iopub.status.idle":"2023-12-13T00:22:18.229886Z","shell.execute_reply.started":"2023-12-13T00:22:18.212143Z","shell.execute_reply":"2023-12-13T00:22:18.228545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# creamy cheese crab dip","metadata":{}},{"cell_type":"code","source":"recommendations('creamy cheese crab dip')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:42:12.042596Z","iopub.execute_input":"2023-12-11T22:42:12.043005Z","iopub.status.idle":"2023-12-11T22:42:12.060105Z","shell.execute_reply.started":"2023-12-11T22:42:12.042963Z","shell.execute_reply":"2023-12-11T22:42:12.058706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# The second monkey bread recipe","metadata":{}},{"cell_type":"code","source":"recommendations('monkey bread1')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:42:24.883029Z","iopub.execute_input":"2023-12-11T22:42:24.883498Z","iopub.status.idle":"2023-12-11T22:42:24.899808Z","shell.execute_reply.started":"2023-12-11T22:42:24.883460Z","shell.execute_reply":"2023-12-11T22:42:24.898518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### HOW to Recommending by a random request/user_input not just using title as an index but the entire df: \nSince we are doing this based on index, we have to find the index of the user input ","metadata":{}},{"cell_type":"markdown","source":"## Word2vec reommendation system","metadata":{}},{"cell_type":"code","source":"from gensim.models import Word2Vec \nfrom nltk.tokenize.toktok import ToktokTokenizer\nimport nltk # natural Langauge Toolkit - \nnltk.download('all',  halt_on_error=False)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:14:09.667670Z","iopub.execute_input":"2023-12-11T22:14:09.668079Z","iopub.status.idle":"2023-12-11T22:14:35.014356Z","shell.execute_reply.started":"2023-12-11T22:14:09.668046Z","shell.execute_reply":"2023-12-11T22:14:35.013022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = ToktokTokenizer()\ntokenized_recipes=[]\nfor sentence in recipe_small2 :\n    tokens=tokenizer.tokenize(sentence)\n    tokenized_recipes.append(tokens)\nmodel=Word2Vec(sentences=tokenized_recipes,vector_size=500,window=30, epochs=7)","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:50:23.779182Z","iopub.execute_input":"2023-12-11T22:50:23.779607Z","iopub.status.idle":"2023-12-11T22:50:24.587050Z","shell.execute_reply.started":"2023-12-11T22:50:23.779574Z","shell.execute_reply":"2023-12-11T22:50:24.586027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus_feature_w2v=[]\n\nfor recipe_tokens in tokenized_recipes:\n    feature_vector=[model.wv[w] for w in recipe_tokens if w in model.wv]\n    temp_features=sum(feature_vector)/len(recipe_tokens)\n    corpus_feature_w2v.append(temp_features)\ncorpus_feature_w2v=np.array(corpus_feature_w2v)\n\ncorpus_feature_w2v.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:50:27.274800Z","iopub.execute_input":"2023-12-11T22:50:27.275247Z","iopub.status.idle":"2023-12-11T22:50:27.449968Z","shell.execute_reply.started":"2023-12-11T22:50:27.275209Z","shell.execute_reply":"2023-12-11T22:50:27.448627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"similarity_matrix= cosine_similarity(corpus_feature_w2v, corpus_feature_w2v)\nsimilarity_matrix.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:50:30.774611Z","iopub.execute_input":"2023-12-11T22:50:30.775046Z","iopub.status.idle":"2023-12-11T22:50:30.802545Z","shell.execute_reply.started":"2023-12-11T22:50:30.775010Z","shell.execute_reply":"2023-12-11T22:50:30.800961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def recommendations(user_input, topk=10):\n    idx=indices1[user_input]\n    sim_values=similarity_matrix[idx]\n    sim_values=list(enumerate(sim_values))\n    sim_values=sorted(sim_values, key=lambda x:x[1], reverse=True)\n    content_index=[i[0] for i in sim_values[1:topk+1]]\n    recom = recipe_small[['new_title', 'ingredients', 'directions', 'link']].iloc[content_index, :]\n    return recom","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:50:35.458482Z","iopub.execute_input":"2023-12-11T22:50:35.458866Z","iopub.status.idle":"2023-12-11T22:50:35.466834Z","shell.execute_reply.started":"2023-12-11T22:50:35.458835Z","shell.execute_reply":"2023-12-11T22:50:35.465402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  old time bread pudding","metadata":{}},{"cell_type":"code","source":"recommendations('old time bread pudding')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:50:40.534858Z","iopub.execute_input":"2023-12-11T22:50:40.535259Z","iopub.status.idle":"2023-12-11T22:50:40.552121Z","shell.execute_reply.started":"2023-12-11T22:50:40.535227Z","shell.execute_reply":"2023-12-11T22:50:40.551197Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# baked beans","metadata":{}},{"cell_type":"code","source":"#recommendations('bake bean')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T23:53:00.649157Z","iopub.execute_input":"2023-12-11T23:53:00.649663Z","iopub.status.idle":"2023-12-11T23:53:00.655962Z","shell.execute_reply.started":"2023-12-11T23:53:00.649625Z","shell.execute_reply":"2023-12-11T23:53:00.654438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# monkey bread","metadata":{}},{"cell_type":"code","source":"recommendations('monkey bread')","metadata":{"execution":{"iopub.status.busy":"2023-12-11T22:51:00.627033Z","iopub.execute_input":"2023-12-11T22:51:00.627486Z","iopub.status.idle":"2023-12-11T22:51:00.644164Z","shell.execute_reply.started":"2023-12-11T22:51:00.627439Z","shell.execute_reply":"2023-12-11T22:51:00.642774Z"},"trusted":true},"execution_count":null,"outputs":[]}]}